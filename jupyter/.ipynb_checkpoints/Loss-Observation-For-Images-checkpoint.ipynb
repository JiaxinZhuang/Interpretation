{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "1. Comparision for batch of images, need to specify the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms                                                                                                                                        \n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../src/\")\n",
    "sys.path.append(\"../../\")\n",
    "import model\n",
    "from datasets import imagenet\n",
    "from loss import FileterLoss\n",
    "import config\n",
    "#from utils.function import init_logging, init_environment, preprocess_image,\\\n",
    "#         recreate_image, get_lr, save_image\n",
    "from aux.utils import obtain_features_map, load_imgs, zscore, extract_valid\n",
    "from aux.visualization import visualize_features_map_for_comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.Path\n",
    "src_image_path = \"./test_data/src.jpeg\"\n",
    "dst_image_path = \"./test_data/dst.jpeg\"\n",
    "original_image_path = \"./test_data/origin.jpeg\"\n",
    "resume = \"037-0\"\n",
    "model_dir = \"../saved/models/\"\n",
    "backbone = \"vgg16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Load Image & preprocess the imgs\n",
    "original_image = np.array(Image.open(original_image_path).convert(\"RGB\")).astype(\"float32\")\n",
    "src_image = np.array(Image.open(src_image_path).convert(\"RGB\")).astype(\"float32\")\n",
    "dst_image = np.array(Image.open(dst_image_path).convert(\"RGB\")).astype(\"float32\")\n",
    "imgs = np.array([original_image, src_image, dst_image])\n",
    "imgs = imgs / 255.0\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "optimized_data = zscore(imgs, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Use original fc\n",
      "Resume from model from exp: 037 at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = model.Network(backbone=backbone, num_classes=1000)\n",
    "net.to(device)\n",
    "# resume from model\n",
    "resume_exp = resume.split(\"-\")[0]\n",
    "resume_epoch = resume.split(\"-\")[1]\n",
    "print(\"Resume from model from exp: {} at epoch {}\".format(resume_exp, resume_epoch))\n",
    "resume_path = os.path.join(model_dir, str(resume_exp), str(resume_epoch))\n",
    "ckpt = torch.load(resume_path, map_location=device)\n",
    "net.load_state_dict(ckpt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter\n",
    "selected_layer = 3 \n",
    "selected_filter = 15 \n",
    "mode = \"keep\"\n",
    "inter = True\n",
    "rho = 0\n",
    "regularization = \"L1\"\n",
    "smoothing = \"TotalVariation\"\n",
    "regular_ex = 1\n",
    "alpha = 100.0\n",
    "beta = 1.0\n",
    "gamma = 1.0\n",
    "delta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.Calculate Loss\n",
    "criterion = FileterLoss(net, selected_layer, selected_filter, mode,\n",
    "                        inter=inter, rho=rho,\n",
    "                        regularization=regularization,\n",
    "                        smoothing=smoothing, p=regular_ex, _print=print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter\n",
      "tensor(204.1762, grad_fn=<MeanBackward0>)\n",
      "tensor(2415.5542, grad_fn=<MeanBackward0>)\n",
      "tensor(204787.3125)\n",
      "tensor(22331.4805)\n"
     ]
    }
   ],
   "source": [
    "# optimized img-1\n",
    "processed_inputs = torch.unsqueeze(torch.tensor(optimized_data[0]), 0)\n",
    "original_inputs = torch.unsqueeze(torch.tensor(optimized_data[1]), 0)\n",
    "selected_filter_loss, rest_fileter_loss, regularization_loss,  \\\n",
    "      smoothing_loss = criterion(processed_inputs, original_inputs)\n",
    "loss = alpha * selected_filter_loss + beta * rest_fileter_loss + \\\n",
    "    gamma * regularization_loss + delta * smoothing_loss\n",
    "print(selected_filter_loss)\n",
    "print(rest_fileter_loss)\n",
    "print(regularization_loss)\n",
    "print(smoothing_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter\n",
      "tensor(208.7141, grad_fn=<MeanBackward0>)\n",
      "tensor(6815.8491, grad_fn=<MeanBackward0>)\n",
      "tensor(204787.3125)\n",
      "tensor(22331.4805)\n"
     ]
    }
   ],
   "source": [
    "# optimized img-2\n",
    "processed_inputs = torch.unsqueeze(torch.tensor(optimized_data[0]), 0)\n",
    "original_inputs = torch.unsqueeze(torch.tensor(optimized_data[2]), 0)\n",
    "selected_filter_loss, rest_fileter_loss, regularization_loss,  \\\n",
    "      smoothing_loss = criterion(processed_inputs, original_inputs)\n",
    "loss = alpha * selected_filter_loss + beta * rest_fileter_loss + \\\n",
    "    gamma * regularization_loss + delta * smoothing_loss\n",
    "print(selected_filter_loss)\n",
    "print(rest_fileter_loss)\n",
    "print(regularization_loss)\n",
    "print(smoothing_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Difference images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
